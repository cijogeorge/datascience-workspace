{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shake detection\n",
    "\n",
    "### Building a classifier for detecting Shake gestures using data from phone sensors\n",
    "\n",
    "#### Problem and dataset description\n",
    "\n",
    "The goal is to create a classifier that can be implemented easily on streaming sensor data to detect Shake gesture.\n",
    "\n",
    "There are 3 data sets:\n",
    "\n",
    "1. a.lbl.csv, a.sensor.csv\n",
    "2. m.lbl.csv, m.sensor.csv\n",
    "3. p.lbl.csv, p.sensor.csv\n",
    "\n",
    "Each set has two files: **sensor.csv** and **lbl.csv**.\n",
    "\n",
    "**sensor.csv** has data from various phone sensors with the below information as columns:\n",
    "\n",
    "* **timestamp(ms)**: Unix Timestamp in milliseconds.\n",
    "* **acceleration_x(g)**: Acceleration along phone's x axis (measured in g)\n",
    "* **acceleration_y(g)**: Acceleration along phone's y axis (measured in g)\n",
    "* **acceleration_z(g)**: Acceleration along phone's z axis (measured in g)\n",
    "* **roll(rad)**: Roll angle representing phone's attitude (orientation)\n",
    "* **pitch(rad)**: Pitch angle representing phone's attitude (orientation)\n",
    "* **yaw(rad)**: Yaw angle representing phone's attitude (orientation)\n",
    "* **angular_velocity_x(rad/sec)**: Angular velocity around x axis (passing thru phones center along smaller edge)\n",
    "* **angular_velocity_y(rad/sec)**: Angular velocity around y axis (passing thru phones center along longer edge)\n",
    "* **angular_velocity_z(rad/sec)**: Angular velocity around z axis (passing thru phones center emerging out of phone )\n",
    "\n",
    "The data is recorded every 100 ms (10Hz frequency).\n",
    "\n",
    "**lbl.csv** contains 2 columes:\n",
    "\n",
    "* **timestamp(ms)**: Unix Timestamp in milliseconds.\n",
    "* **label**: 2 possible types of labels as given below.\n",
    "\n",
    "Possible types of labels in **lbl.csv**:\n",
    "\n",
    "1. Label **0** : Represents start of Shake gesture\n",
    "2. Label **1** : Indicates end of Shake gesture\n",
    "\n",
    "The Goal is to create a classifier that can be implemented easily on streaming sensor data to detect Shake gesture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting configuration and loading libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading required package: lattice\n",
      "Loading required package: ggplot2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"Done\"\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "trainSetRatio <- 1\n",
    "modelNameStr <- \"shakeDetectionClassifier\"\n",
    "classLabel <- \"label\"\n",
    "\n",
    "# Load required libraries\n",
    "library(caret)\n",
    "#library(rattle)\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading input datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"Reading input datasets ...\"\n",
      "[1] \"Summary of sensor data:\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       " timestamp.ms.       acceleration_x.g.  acceleration_y.g.   acceleration_z.g. \n",
       " Min.   :1.397e+12   Min.   :-0.57910   Min.   :-0.974300   Min.   :-1.68920  \n",
       " 1st Qu.:1.397e+12   1st Qu.: 0.00150   1st Qu.: 0.003100   1st Qu.:-0.00860  \n",
       " Median :1.397e+12   Median : 0.00320   Median : 0.005100   Median :-0.00600  \n",
       " Mean   :1.397e+12   Mean   : 0.00452   Mean   : 0.005245   Mean   :-0.00663  \n",
       " 3rd Qu.:1.397e+12   3rd Qu.: 0.00500   3rd Qu.: 0.007000   3rd Qu.:-0.00220  \n",
       " Max.   :1.397e+12   Max.   : 1.50560   Max.   : 1.282100   Max.   : 0.97860  \n",
       "   roll.rad.         pitch.rad.         yaw.rad.      \n",
       " Min.   :-0.3519   Min.   :-1.4522   Min.   :-3.1409  \n",
       " 1st Qu.: 0.3133   1st Qu.:-0.6624   1st Qu.: 0.8498  \n",
       " Median : 0.3193   Median :-0.6362   Median : 1.2403  \n",
       " Mean   : 0.6234   Mean   :-0.6615   Mean   : 1.2405  \n",
       " 3rd Qu.: 1.3728   3rd Qu.:-0.6143   3rd Qu.: 1.9628  \n",
       " Max.   : 2.4940   Max.   :-0.3796   Max.   : 3.1401  \n",
       " angular_velocity_x.rad.sec. angular_velocity_y.rad.sec.\n",
       " Min.   :-5.563200           Min.   :-5.563200          \n",
       " 1st Qu.:-0.003200           1st Qu.:-0.003200          \n",
       " Median : 0.000300           Median : 0.000300          \n",
       " Mean   :-0.001074           Mean   :-0.001074          \n",
       " 3rd Qu.: 0.003100           3rd Qu.: 0.003100          \n",
       " Max.   : 2.852200           Max.   : 2.852200          \n",
       " angular_velocity_z.rad.sec.\n",
       " Min.   :-3.0758000         \n",
       " 1st Qu.:-0.0033000         \n",
       " Median :-0.0001000         \n",
       " Mean   : 0.0000844         \n",
       " 3rd Qu.: 0.0033000         \n",
       " Max.   : 1.4193000         "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       " timestamp.ms.       acceleration_x.g.  acceleration_y.g.  acceleration_z.g.\n",
       " Min.   :1.397e+12   Min.   :-7.34380   Min.   :-5.75090   Min.   :-7.3489  \n",
       " 1st Qu.:1.397e+12   1st Qu.:-0.03190   1st Qu.:-0.02270   1st Qu.:-0.0942  \n",
       " Median :1.397e+12   Median : 0.00180   Median : 0.00290   Median :-0.0025  \n",
       " Mean   :1.397e+12   Mean   : 0.02902   Mean   : 0.07933   Mean   :-0.0314  \n",
       " 3rd Qu.:1.397e+12   3rd Qu.: 0.06980   3rd Qu.: 0.07100   3rd Qu.: 0.0564  \n",
       " Max.   :1.397e+12   Max.   : 7.66600   Max.   : 8.60060   Max.   : 4.2670  \n",
       "   roll.rad.         pitch.rad.         yaw.rad.     \n",
       " Min.   :-3.1208   Min.   :-1.5570   Min.   :-3.141  \n",
       " 1st Qu.:-0.4437   1st Qu.: 0.0083   1st Qu.:-1.915  \n",
       " Median :-0.0766   Median : 0.2682   Median :-1.514  \n",
       " Mean   :-0.1789   Mean   : 0.2789   Mean   :-0.296  \n",
       " 3rd Qu.:-0.0177   3rd Qu.: 0.4857   3rd Qu.: 2.206  \n",
       " Max.   : 3.1363   Max.   : 1.5416   Max.   : 3.141  \n",
       " angular_velocity_x.rad.sec. angular_velocity_y.rad.sec.\n",
       " Min.   :-28.17080           Min.   :-28.17080          \n",
       " 1st Qu.: -0.12020           1st Qu.: -0.12020          \n",
       " Median :  0.00000           Median :  0.00000          \n",
       " Mean   :  0.02961           Mean   :  0.02961          \n",
       " 3rd Qu.:  0.15650           3rd Qu.:  0.15650          \n",
       " Max.   : 22.08510           Max.   : 22.08510          \n",
       " angular_velocity_z.rad.sec.\n",
       " Min.   :-18.35650          \n",
       " 1st Qu.: -0.10980          \n",
       " Median :  0.00020          \n",
       " Mean   :  0.01388          \n",
       " 3rd Qu.:  0.08440          \n",
       " Max.   : 29.83470          "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       " timestamp.ms.       acceleration_x.g.  acceleration_y.g.  acceleration_z.g. \n",
       " Min.   :1.397e+12   Min.   :-3.01870   Min.   :-2.91270   Min.   :-2.69040  \n",
       " 1st Qu.:1.397e+12   1st Qu.:-0.00330   1st Qu.:-0.00270   1st Qu.: 0.00560  \n",
       " Median :1.397e+12   Median :-0.00020   Median :-0.00020   Median : 0.00860  \n",
       " Mean   :1.397e+12   Mean   :-0.05438   Mean   : 0.02223   Mean   : 0.02437  \n",
       " 3rd Qu.:1.397e+12   3rd Qu.: 0.00180   3rd Qu.: 0.00240   3rd Qu.: 0.01320  \n",
       " Max.   :1.397e+12   Max.   : 2.34530   Max.   : 2.76790   Max.   : 2.58660  \n",
       "   roll.rad.          pitch.rad.          yaw.rad.      \n",
       " Min.   :-3.14120   Min.   :-1.55500   Min.   :-3.1411  \n",
       " 1st Qu.:-0.16330   1st Qu.:-0.11130   1st Qu.:-1.0503  \n",
       " Median : 0.00510   Median :-0.10080   Median : 1.5927  \n",
       " Mean   :-0.04836   Mean   :-0.09124   Mean   : 0.7227  \n",
       " 3rd Qu.: 0.00570   3rd Qu.: 0.02470   3rd Qu.: 2.0301  \n",
       " Max.   : 3.13920   Max.   : 1.51790   Max.   : 3.1414  \n",
       " angular_velocity_x.rad.sec. angular_velocity_y.rad.sec.\n",
       " Min.   :-13.145700          Min.   :-13.145700         \n",
       " 1st Qu.: -0.006300          1st Qu.: -0.006300         \n",
       " Median :  0.000000          Median :  0.000000         \n",
       " Mean   : -0.002773          Mean   : -0.002773         \n",
       " 3rd Qu.:  0.006500          3rd Qu.:  0.006500         \n",
       " Max.   : 19.212900          Max.   : 19.212900         \n",
       " angular_velocity_z.rad.sec.\n",
       " Min.   :-25.219800         \n",
       " 1st Qu.: -0.005700         \n",
       " Median : -0.000100         \n",
       " Mean   : -0.004574         \n",
       " 3rd Qu.:  0.005800         \n",
       " Max.   : 22.101600         "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"Summary of label data:\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       " timestamp.ms.       label..0.start.1.end.2.cancel.\n",
       " Min.   :1.397e+12   Min.   :0.0                   \n",
       " 1st Qu.:1.397e+12   1st Qu.:0.0                   \n",
       " Median :1.397e+12   Median :0.5                   \n",
       " Mean   :1.397e+12   Mean   :0.5                   \n",
       " 3rd Qu.:1.397e+12   3rd Qu.:1.0                   \n",
       " Max.   :1.397e+12   Max.   :1.0                   "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       " timestamp.ms.       label..0.start.1.end.2.cancel.\n",
       " Min.   :1.397e+12   Min.   :0.0                   \n",
       " 1st Qu.:1.397e+12   1st Qu.:0.0                   \n",
       " Median :1.397e+12   Median :0.5                   \n",
       " Mean   :1.397e+12   Mean   :0.5                   \n",
       " 3rd Qu.:1.397e+12   3rd Qu.:1.0                   \n",
       " Max.   :1.397e+12   Max.   :1.0                   "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       " timestamp.ms.       label..0.start.1.end.2.cancel.\n",
       " Min.   :1.397e+12   Min.   :0.0                   \n",
       " 1st Qu.:1.397e+12   1st Qu.:0.0                   \n",
       " Median :1.397e+12   Median :0.5                   \n",
       " Mean   :1.397e+12   Mean   :0.5                   \n",
       " 3rd Qu.:1.397e+12   3rd Qu.:1.0                   \n",
       " Max.   :1.397e+12   Max.   :1.0                   "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"Done.\"\n"
     ]
    }
   ],
   "source": [
    "# Reading in the sensor and label dataset\n",
    "print(\"Reading input datasets ...\")\n",
    "\n",
    "a.sensor <- read.csv(\"data/a.sensor.csv\", colClasses=\"numeric\")\n",
    "m.sensor <- read.csv(\"data/m.sensor.csv\", colClasses=\"numeric\")\n",
    "p.sensor <- read.csv(\"data/p.sensor.csv\", colClasses=\"numeric\")\n",
    "\n",
    "a.lbl <- read.csv(\"data/a.lbl.csv\", colClasses=\"numeric\")\n",
    "m.lbl <- read.csv(\"data/m.lbl.csv\", colClasses=\"numeric\")\n",
    "p.lbl <- read.csv(\"data/p.lbl.csv\", colClasses=\"numeric\")\n",
    "\n",
    "print(\"Summary of sensor data:\")\n",
    "summary(a.sensor)\n",
    "summary(m.sensor)\n",
    "summary(p.sensor)\n",
    "\n",
    "print(\"Summary of label data:\")\n",
    "summary(a.lbl)\n",
    "summary(m.lbl)\n",
    "summary(p.lbl)\n",
    "\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing data for analysis\n",
    "\n",
    "#### Data transformations\n",
    "\n",
    "Convert the 'lbl' datasets into 'start_time,end_time' format. This is for easier implementation of the logic for creating a labelled training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"Transforming label dataset into 'start_time,end_time' format ...\"\n",
      "[1] \"Summary of label data after transformation:\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "       V1                  V2           \n",
       " Min.   :1.397e+12   Min.   :1.397e+12  \n",
       " 1st Qu.:1.397e+12   1st Qu.:1.397e+12  \n",
       " Median :1.397e+12   Median :1.397e+12  \n",
       " Mean   :1.397e+12   Mean   :1.397e+12  \n",
       " 3rd Qu.:1.397e+12   3rd Qu.:1.397e+12  \n",
       " Max.   :1.397e+12   Max.   :1.397e+12  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"Done\"\n"
     ]
    }
   ],
   "source": [
    "## Transform label dataset into \"<startime>,<endtime>\" format\n",
    "transformLabelData <- function(lbl){\n",
    "    lbl.modified <- cbind(lbl[lbl[, 2] == 0, 1], lbl[lbl[, 2] == 1, 1])\n",
    "    return(lbl.modified)\n",
    "}\n",
    "\n",
    "## Transforming the label dataset into 'start_time,end_time' format\n",
    "print(\"Transforming label dataset into 'start_time,end_time' format ...\")\n",
    "a.lbl.modified <- transformLabelData(a.lbl)\n",
    "m.lbl.modified <- transformLabelData(m.lbl)                                              \n",
    "p.lbl.modified <- transformLabelData(p.lbl)\n",
    "\n",
    "print(\"Summary of label data after transformation:\")\n",
    "summary(a.lbl.modified)\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating labelled dataset\n",
    "\n",
    "Create labelled training dataset for each of the sensor datasets separately and then combine them. '**apply**' and '**sapply**' functions are used for this and no 'for' loops since 'for' loops are really inefficient in R. The 'apply' functions could also be parallelized using their multicore versions from 'parallel' package in R and even further optimized by multiprocessing or even using frameworks like Spark for large datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"Creating labelled training dataset ...\"\n",
      "[1] \"Summary of labelled training dataset:\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       " acceleration_x.g.  acceleration_y.g.  acceleration_z.g.     roll.rad.      \n",
       " Min.   :-7.34380   Min.   :-5.75090   Min.   :-7.348900   Min.   :-3.1412  \n",
       " 1st Qu.:-0.00110   1st Qu.:-0.00090   1st Qu.:-0.008300   1st Qu.:-0.0073  \n",
       " Median : 0.00170   Median : 0.00290   Median :-0.001100   Median : 0.0173  \n",
       " Mean   :-0.01778   Mean   : 0.02224   Mean   : 0.003558   Mean   : 0.2283  \n",
       " 3rd Qu.: 0.00480   3rd Qu.: 0.00710   3rd Qu.: 0.009400   3rd Qu.: 0.3194  \n",
       " Max.   : 7.66600   Max.   : 8.60060   Max.   : 4.267000   Max.   : 3.1392  \n",
       "   pitch.rad.         yaw.rad.       angular_velocity_x.rad.sec.\n",
       " Min.   :-1.5570   Min.   :-3.1411   Min.   :-28.170800         \n",
       " 1st Qu.:-0.6355   1st Qu.: 0.3465   1st Qu.: -0.004800         \n",
       " Median :-0.1668   Median : 1.3916   Median :  0.000200         \n",
       " Mean   :-0.2924   Mean   : 0.8165   Mean   :  0.002184         \n",
       " 3rd Qu.:-0.0644   3rd Qu.: 2.0131   3rd Qu.:  0.004800         \n",
       " Max.   : 1.5416   Max.   : 3.1414   Max.   : 22.085100         \n",
       " angular_velocity_y.rad.sec. angular_velocity_z.rad.sec.      label       \n",
       " Min.   :-28.170800          Min.   :-25.219800          NO_SHAKE:161319  \n",
       " 1st Qu.: -0.004800          1st Qu.: -0.004700          SHAKE   :  2550  \n",
       " Median :  0.000200          Median : -0.000100                           \n",
       " Mean   :  0.002184          Mean   : -0.000136                           \n",
       " 3rd Qu.:  0.004800          3rd Qu.:  0.004700                           \n",
       " Max.   : 22.085100          Max.   : 29.834700                           "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Create a labelled training set from sensor and label datasets\n",
    "createTrainingDataset <- function(sensor, lbl.modified){\n",
    "    label <- sapply(sensor[, 1], \n",
    "                    function(ts)\n",
    "                        any(apply(lbl.modified, 1, \n",
    "                                  function(shake_ts, test_ts)\n",
    "                                      test_ts >= shake_ts[1] && \n",
    "                                      test_ts <= shake_ts[2], test_ts=ts)))\n",
    "    label[label == TRUE] <- \"SHAKE\"\n",
    "    label[label == FALSE] <- \"NO_SHAKE\"\n",
    "    sensor.labelled <- cbind(sensor[, -1], label)\n",
    "    return(sensor.labelled)\n",
    "}\n",
    "                                  \n",
    "# Create a labelled training datasets\n",
    "print(\"Creating labelled training dataset ...\")\n",
    "a.sensor.labelled <- createTrainingDataset(a.sensor, a.lbl.modified)\n",
    "m.sensor.labelled <- createTrainingDataset(m.sensor, m.lbl.modified)\n",
    "p.sensor.labelled <- createTrainingDataset(p.sensor, p.lbl.modified)\n",
    "\n",
    "# Combining the labelled training datasets\n",
    "sensor.labelled <- rbind(a.sensor.labelled, m.sensor.labelled, p.sensor.labelled)\n",
    "\n",
    "print(\"Summary of labelled training dataset:\")\n",
    "summary(sensor.labelled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating train and test datasets \n",
    "\n",
    "Split the labelled dataset into train and test set based on a pre-defined ratio. For 80:20 split, 80% of the data from each 'class label' is selected randomly for training and the rest of testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"Spliting data into train set and test set ...\"\n",
      "[1] \"Done\"\n"
     ]
    }
   ],
   "source": [
    "# Split data into training and test set\n",
    "trainSet = NULL\n",
    "testSet = NULL\n",
    "\n",
    "print(\"Spliting data into train set and test set ...\")\n",
    "if(trainSetRatio == 1){\n",
    "    trainSet <- sensor.labelled\n",
    "    testSet <- sensor.labelled\n",
    "\n",
    "} else if(trainSetRatio < 1){\n",
    "    inTrain <- createDataPartition(sensor.labelled[, classLabel], p = trainSetRatio, list=FALSE)\n",
    "\n",
    "    trainSet <- sensor.labelled[inTrain,]\n",
    "    testSet <- sensor.labelled[-inTrain,]\n",
    "}\n",
    "    \n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the classifier\n",
    "\n",
    "A '**decision-tree**' based classifier is trained using '**recursive partitioning**' ('rpart' method in R). The choice of using a decision tree based algorithm is because decision trees would be much more efficient for shake detection on incoming streaming data. Decsion trees will be simple to interpret and can be converted into rules for  matching on input data if needed. Choice of 'recursive partitioning'/ 'rpart' from among the available tree based algorithms is arbitrary.\n",
    "\n",
    "#### Hyper-parameter tuning using Grid Search\n",
    "\n",
    "'rpart' has just one hyper-parameter which is the '**complexity parameter**' - 'cp'. The best 'cp' is selected by doing  Grid Search using '**repeated cross-validation**' (10 fold cross validation repeated 10 times) on a range of 'cp' values. The 'cp' value which gives the best 'F1-Score' from the 'repeated cross-validation' is selected. The reasoning behind using '**F1-score**' here is explained below in 'Design Considerations'.\n",
    "\n",
    "#### Design Considerations:  Data is heavily skewed!\n",
    "\n",
    "After creating a labelled dataset for training the classifier, it is observed that **only 1.5% of the training data consists of measurements while the phone was 'Shaking' (Positive class for the Classifier)**. This means that the data is heavily skewed. This makes the classification problem more like a '**needle in a haystack**' problem. In this case, the probability of even a random classifier giving a lot of 'True Negatives' is very high. This will affect our evaluation of the classifier if we are not careful enough. For example, **'Accuracy' of the classifier of even a random classifier will be very high because 'True Negatives' will be high**. Hence we have to use evaluation metrics like Precision (Positive Predictive Value) and Recall (Sensitivity/  True Positive Rate) to evaluate the classifier or even selecting parameters in the training phase. Precision, Recall, F1-Score or some other similar 'True Negative' independent metrics should be used to select hyper-parameters values for a classification algorithm for solving such a problem. Here, I've used 'recursive partitioning' algorithm to build a classifier and used 'repeated cross-validation' to select the 'complexity parameter' that maximizes 'F1-Score'. **A custom function is written for this which is used by 'trainControl' in 'caret' package in R for training the classifier**.\n",
    "\n",
    "**Note:** Another approach to handle skewed datasets is to do up/ down sampling i.e., either increase the number of positive samples by replication (as is or with slight variations) or decrease th number of negative samples by taking only a random subset of the negative class set with size roughly equal to the size of the positive class set. Code for this is commented out as it is not used in this evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"Predictors:\"\n",
      "[1] \"acceleration_x.g.\"           \"acceleration_y.g.\"          \n",
      "[3] \"acceleration_z.g.\"           \"roll.rad.\"                  \n",
      "[5] \"pitch.rad.\"                  \"yaw.rad.\"                   \n",
      "[7] \"angular_velocity_x.rad.sec.\" \"angular_velocity_y.rad.sec.\"\n",
      "[9] \"angular_velocity_z.rad.sec.\"\n",
      "[1] \"Training the classifier ...\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading required package: rpart\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CART \n",
      "\n",
      "163869 samples\n",
      "     9 predictor\n",
      "     2 classes: 'NO_SHAKE', 'SHAKE' \n",
      "\n",
      "No pre-processing\n",
      "Resampling: Cross-Validated (10 fold, repeated 10 times) \n",
      "Summary of sample sizes: 147482, 147482, 147482, 147482, 147482, 147482, ... \n",
      "Resampling results across tuning parameters:\n",
      "\n",
      "  cp      Precision  Recall     F1-Score \n",
      "  0.0020  0.6831579  0.2500392  0.3648623\n",
      "  0.0025  0.6964482  0.2358824  0.3513641\n",
      "  0.0030  0.7063529  0.2282745  0.3436809\n",
      "  0.0035  0.7078026  0.2204706  0.3350840\n",
      "  0.0040  0.7108699  0.2136863  0.3273557\n",
      "  0.0045  0.7106366  0.2083137  0.3208771\n",
      "  0.0050  0.7061223  0.2041176  0.3153718\n",
      "  0.0055  0.7026750  0.1976078  0.3070021\n",
      "  0.0060  0.7053750  0.1949412  0.3039943\n",
      "  0.0065  0.7053563  0.1916471  0.2998883\n",
      "\n",
      "F1-Score was used to select the optimal model using  the largest value.\n",
      "The final value used for the model was cp = 0.002.\n",
      "[1] \"Done\"\n"
     ]
    }
   ],
   "source": [
    "# Create classLabel ~ predictor1 + predictor2 .. string\n",
    "predictors <- colnames(sensor.labelled)[-ncol(sensor.labelled)]\n",
    "\n",
    "# Print predictors\n",
    "print(\"Predictors:\")\n",
    "print(predictors)\n",
    "\n",
    "# Uncomment relevant parts of below code for up/ down sampling training data\n",
    "\n",
    "# print(paste(\"Before up/ down sampling: nrow(trainSet):\", \n",
    "#       nrow(trainSet), \"nrow(testSet):\", nrow(testSet)))\n",
    "\n",
    "# trainSet <- upSample(trainSet[, predictors], \n",
    "#                     as.factor(trainSet[, classLabel]), \n",
    "#                     list=FALSE, yname=classLabel)\n",
    "# trainSet <- downSample(trainSet[, predictors], \n",
    "#                        as.factor(trainSet[, classLabel]), \n",
    "#                        list=FALSE, yname=classLabel)\n",
    "\n",
    "# print(paste (\"After up/ down sampling: nrow(trainSet):\", nrow(trainSet), \"nrow(testSet):\", nrow(testSet)))\n",
    "\n",
    "# Training the classifier\n",
    "print(\"Training the classifier ...\")\n",
    "\n",
    "# Custom summary function for trainControl\n",
    "trainControlSumFuncCustom <- function(data, lev=NULL, model=NULL){\n",
    "    if(!all(levels(data[, \"pred\"]) == levels(data[, \"obs\"]))){\n",
    "        print(\"ERROR: Levels of observed and predicted data do not match\")\n",
    "        q()\n",
    "    }\n",
    "    \n",
    "    precision <- posPredValue(data[, \"pred\"], data[, \"obs\"], \n",
    "                              positive=\"SHAKE\")\n",
    "    \n",
    "    recall <- sensitivity(data[, \"pred\"], data[, \"obs\"], \n",
    "                          positive=\"SHAKE\")\n",
    "    \n",
    "    f1score <- 2 * ((precision * recall) / (precision + recall))\n",
    "\n",
    "    out <- c(precision, recall, f1score)\n",
    "    names(out) <- c(\"Precision\", \"Recall\", \"F1-Score\")\n",
    "\n",
    "    return(out)\n",
    "}\n",
    "\n",
    "fitControl <- trainControl(method=\"repeatedcv\", \n",
    "                           number=10, repeats=10, \n",
    "                           classProbs=TRUE, \n",
    "                           summaryFunction=trainControlSumFuncCustom, \n",
    "                           selectionFunction=\"best\")\n",
    "\n",
    "rpart.grid <- expand.grid(cp=c(0.002, 0.0025, 0.003, 0.0035, 0.004, 0.0045, 0.005, 0.0055, 0.006, 0.0065))\n",
    "\n",
    "rpartFit <- train(trainSet[, predictors], \n",
    "                  as.factor(as.character(trainSet[, classLabel])), \n",
    "                  method=\"rpart\", \n",
    "                  trControl=fitControl, \n",
    "                  metric=\"F1-Score\", \n",
    "                  tuneGrid = rpart.grid)\n",
    "\n",
    "# Print the classifier details\n",
    "print(rpartFit)\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "\n",
    "The classifier model is evaluated using the test dataset. Output includes predicted probabilies for each class for each sample data, confusion matrix, and other standard evaluation metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"Confusion matrix\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Confusion Matrix and Statistics\n",
       "\n",
       "          Reference\n",
       "Prediction NO_SHAKE  SHAKE\n",
       "  NO_SHAKE   161071   1797\n",
       "  SHAKE         248    753\n",
       "                                         \n",
       "               Accuracy : 0.9875         \n",
       "                 95% CI : (0.987, 0.9881)\n",
       "    No Information Rate : 0.9844         \n",
       "    P-Value [Acc > NIR] : < 2.2e-16      \n",
       "                                         \n",
       "                  Kappa : 0.419          \n",
       " Mcnemar's Test P-Value : < 2.2e-16      \n",
       "                                         \n",
       "            Sensitivity : 0.295294       \n",
       "            Specificity : 0.998463       \n",
       "         Pos Pred Value : 0.752248       \n",
       "         Neg Pred Value : 0.988967       \n",
       "             Prevalence : 0.015561       \n",
       "         Detection Rate : 0.004595       \n",
       "   Detection Prevalence : 0.006109       \n",
       "      Balanced Accuracy : 0.646878       \n",
       "                                         \n",
       "       'Positive' Class : SHAKE          \n",
       "                                         "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"Done.\"\n"
     ]
    }
   ],
   "source": [
    "# Predict classes without probabilities on test data\n",
    "predClasses <- predict(rpartFit, newdata=testSet[, predictors])\n",
    "\n",
    "# Print prediction details on test data\n",
    "# print(\"Predictions:\")\n",
    "# print(predClasses)\n",
    "\n",
    "# Predict classes with probabilities on test data\n",
    "# predClassesProbs <- predict(rpartFit, newdata=testSet[, predictors], type=\"prob\")\n",
    "\n",
    "# Print prediction details on test data with probabilities\n",
    "# print(\"Class Probabilities:\")\n",
    "# predClassesProbs\n",
    "\n",
    "# Print confusion matrix\n",
    "print(\"Confusion matrix\")\n",
    "confusionMatrix(data=predClasses, positive=\"SHAKE\", as.factor(testSet[, classLabel]))\n",
    "\n",
    "# Plot classification tree\n",
    "# fancyRpartPlot(rpartFit$finalModel)\n",
    "\n",
    "print(\"Done.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
